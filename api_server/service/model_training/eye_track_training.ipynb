{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heatmap images have been saved to: D:/GitProjects/Datasets/ImageData/heatmaps/valid/non_autistic\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Function to apply heatmap to an image\n",
    "def apply_heatmap(image_path, output_path):\n",
    "    # Read the image\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "    # Normalize the image to the range 0-255\n",
    "    img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "    # Apply the heatmap\n",
    "    heatmap = cv2.applyColorMap(img, cv2.COLORMAP_JET)\n",
    "    \n",
    "    # Save the heatmap image\n",
    "    cv2.imwrite(output_path, heatmap)\n",
    "\n",
    "# Path to the folder containing raw images\n",
    "input_folder = 'D:/GitProjects/Datasets/ImageData/valid/non_autistic'\n",
    "# Path to the folder to save heatmap images\n",
    "output_folder = 'D:/GitProjects/Datasets/ImageData/heatmaps/valid/non_autistic'\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Process each image in the input folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "        \n",
    "        apply_heatmap(input_path, output_path)\n",
    "\n",
    "print(\"Heatmap images have been saved to:\", output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import splitfolders\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy(\"float32\")\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.layers import Dense,Activation,Dropout,Conv2D,MaxPooling2D,BatchNormalization,Flatten\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import matplotlib.image as mping\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a model\n",
    "def create_model(model_name):\n",
    "    input_shape = (224, 224, 3)\n",
    "    base_model = None\n",
    "    \n",
    "    #if model_name == 'EfficientNetB4':\n",
    "    base_model = EfficientNetB4(include_top=False, weights=\"imagenet\", input_shape=input_shape, pooling='max')\n",
    "    #elif model_name == 'VGG19':\n",
    "    #    base_model = VGG19(include_top=False, weights=\"imagenet\", input_shape=input_shape, pooling='max')\n",
    "    #elif model_name == 'NASNetMobile':\n",
    "    #    base_model = NASNetMobile(include_top=False, weights=\"imagenet\", input_shape=input_shape, pooling='max')\n",
    "    #elif model_name == 'Xception':\n",
    "    #    base_model = Xception(include_top=False, weights=\"imagenet\", input_shape=input_shape, pooling='max')\n",
    "\n",
    "    x = base_model.output\n",
    "    x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.002)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(rate=0.45, seed=42)(x)\n",
    "    output = Dense(class_count, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    model.compile(tf.keras.optimizers.Adam(learning_rate=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Evaluate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train and evaluate a model\n",
    "def train_and_evaluate_model(model, train_data, val_data, test_data, model_name):\n",
    "    early_stopper = tf.keras.callbacks.EarlyStopping(patience=5, min_delta=0.01, verbose=1)\n",
    "    history = model.fit(train_data, epochs=60, validation_data=val_data, callbacks=[early_stopper])\n",
    "\n",
    "    test_loss, test_accuracy = model.evaluate(test_data)\n",
    "    print(f\"Test Accuracy for {model_name}:\", test_accuracy)\n",
    "\n",
    "    test_data_array = []\n",
    "    labels_array = []\n",
    "    for images, labels in test_data:\n",
    "        test_data_array.append(images.numpy())\n",
    "        labels_array.append(labels.numpy())\n",
    "\n",
    "    X_test = np.concatenate(test_data_array, axis=0)\n",
    "    y_test = np.concatenate(labels_array, axis=0)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_pred = tf.keras.utils.to_categorical(y_pred, num_classes=2)\n",
    "\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, cmap='crest', annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'Confusion Matrix for {model_name}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 2936 files [00:01, 2411.88 files/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2348 files belonging to 2 classes.\n",
      "Found 296 files belonging to 2 classes.\n",
      "Found 292 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Path to the consolidated data\n",
    "loc = \"D:/GitProjects/Datasets/ImageData/consolidated\"\n",
    "output_loc = \"D:/GitProjects/Datasets/ImageData/\"\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(os.path.join(output_loc, 'output'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_loc, 'output/train'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_loc, 'output/val'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_loc, 'output/test'), exist_ok=True)\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "splitfolders.ratio(loc, output=os.path.join(output_loc, 'output'), seed=42, ratio=(0.80, 0.1, 0.1))\n",
    "\n",
    "# Load data\n",
    "train_dir = os.path.join(output_loc, \"output/train\")\n",
    "test_dir = os.path.join(output_loc, \"output/test\")\n",
    "val_dir = os.path.join(output_loc, \"output/val\")\n",
    "\n",
    "train_data = image_dataset_from_directory(train_dir, batch_size=32, image_size=(224, 224), label_mode='categorical', shuffle=True, seed=42)\n",
    "test_data = image_dataset_from_directory(test_dir, batch_size=32, image_size=(224, 224), label_mode='categorical', shuffle=False, seed=42)\n",
    "val_data = image_dataset_from_directory(val_dir, batch_size=32, image_size=(224, 224), label_mode='categorical', shuffle=False, seed=42)\n",
    "\n",
    "class_names = train_data.class_names\n",
    "class_count = len(class_names)\n",
    "\n",
    "# Train and evaluate models\n",
    "model_names = ['EfficientNetB4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    model = create_model(model_name)\n",
    "    train_and_evaluate_model(model, train_data, val_data, test_data, model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
